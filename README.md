# ğŸ§  ImplementaÃ§Ã£o de Rede Neural MLP com Backpropagation

## ğŸ“š Trabalho de InteligÃªncia Artificial - 2Âº Bimestre

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o de uma ferramenta para o **treinamento de uma Rede Neural Multilayer Perceptron (MLP)** utilizando o algoritmo **Backpropagation**, desenvolvida como parte do 2Âº trabalho da disciplina de InteligÃªncia Artificial.

---

## ğŸ“Œ DescriÃ§Ã£o do Projeto

O projeto consiste na criaÃ§Ã£o de uma rede neural capaz de aprender a partir de um conjunto de dados de entrada, fornecido por arquivos `.csv`, com o objetivo de realizar **classificaÃ§Ã£o supervisionada** em um conjunto de 5 classes de saÃ­da.


---

## âš™ï¸ Funcionalidades

- Leitura de arquivos `.csv` para treinamento e teste;
- DefiniÃ§Ã£o do **nÃºmero de neurÃ´nios na camada oculta** (com sugestÃµes baseadas na mÃ©dia aritmÃ©tica ou geomÃ©trica);
- Escolha da **funÃ§Ã£o de ativaÃ§Ã£o** para os neurÃ´nios:
  - Linear
  - LogÃ­stica (Sigmoid)
  - Tangente HiperbÃ³lica (Tanh)
- DefiniÃ§Ã£o da **taxa de aprendizagem (Î·)** entre 0 e 1;
- Escolha do **critÃ©rio de parada**:
  - Por nÃºmero mÃ¡ximo de Ã©pocas (iteraÃ§Ãµes)
  - Por erro mÃ­nimo atingido
- **GeraÃ§Ã£o da Matriz de ConfusÃ£o** para avaliaÃ§Ã£o de desempenho da rede apÃ³s o teste.

---

## ğŸ§® FÃ³rmulas Importantes

### Erro na Camada de SaÃ­da:

- **Sigmoid**:  
  `ErroG = (Desejado - iG) * (iG * (1 - iG))`

- **Tangente HiperbÃ³lica**:  
  `ErroG = (Desejado - iG) * (1 - iGÂ²)`

### Erro na Camada Oculta:

- **Sigmoid**:  
  `ErroC = (ErroG * peso) * (iC * (1 - iC))`

- **Tangente HiperbÃ³lica**:  
  `ErroC = (ErroG * peso) * (1 - iCÂ²)`



